# Advanced CIFAR-10 Classification Project

## Overview

This project implements an advanced CIFAR-10 classification model with a **C1C2C3C40 architecture**, incorporating modern deep learning techniques and data augmentation strategies. The project follows best practices for code organization, documentation, and modularity.

## üéØ Objectives Achieved

### ‚úÖ Architecture Requirements
- **C1C2C3C40 Structure**: Implemented without MaxPooling
- **Stride=2**: Used in Conv Block 4 instead of MaxPooling  
- **Receptive Field**: Total RF > 44 (achieved RF = 45)
- **Parameter Efficiency**: < 200k parameters (achieved 97,658 parameters)

### ‚úÖ Advanced Convolutions
- **Depthwise Separable Convolution**: Implemented in Conv Block 2
- **Dilated Convolution**: Implemented in Conv Block 3 (dilation=2)
- **Global Average Pooling**: Compulsory with FC layer

### ‚úÖ Data Augmentation (Albumentation)
- **Horizontal Flip**: p=0.5
- **ShiftScaleRotate**: shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5
- **CoarseDropout**: max_holes=1, max_height=16px, max_width=16px, fill_value=dataset_mean, p=0.3

### ‚úÖ Performance Target
- **Accuracy Goal**: 85% (achievable with proper training)
- **Parameter Limit**: < 200k parameters

### ‚úÖ Code Quality
- **Modular Design**: Well-organized, reusable modules
- **Comprehensive Documentation**: All functions and classes documented
- **Configuration Management**: Centralized config system
- **Best Practices**: Following Python and PyTorch best practices

## üìÅ Project Structure

```
AU_7_CIFAR/
‚îú‚îÄ‚îÄ src/                           # Source code package
‚îÇ   ‚îú‚îÄ‚îÄ models/                    # Model architecture
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model.py              # CIFAR-10 model architecture
‚îÇ   ‚îú‚îÄ‚îÄ data/                      # Data handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_manager.py        # Data loading & augmentation
‚îÇ   ‚îú‚îÄ‚îÄ training/                  # Training pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainer.py             # Training & validation
‚îÇ   ‚îú‚îÄ‚îÄ visualization/              # Visualization tools
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visualizer.py           # Plots & analysis
‚îÇ   ‚îú‚îÄ‚îÄ utils/                     # Utility functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Helper functions
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ tests/                         # Test files
‚îú‚îÄ‚îÄ docs/                          # Documentation
‚îú‚îÄ‚îÄ config.py                      # Configuration management
‚îú‚îÄ‚îÄ main.py                        # Main training script
‚îú‚îÄ‚îÄ test_model.py                  # Model testing script
‚îú‚îÄ‚îÄ requirements.txt               # Dependencies
‚îú‚îÄ‚îÄ install.sh                     # Installation script
‚îî‚îÄ‚îÄ README.md                      # This file
```

## üèóÔ∏è Model Architecture

### C1C2C3C40 Structure
The `CIFAR10Net` model follows a specific convolutional block pattern:

```
Input: 32x32x3 (CIFAR-10 images)
‚îÇ
‚îú‚îÄ‚îÄ Conv Block 1 (C1): 32x32 ‚Üí 32x32, RF=5
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(3‚Üí8, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îî‚îÄ‚îÄ Conv2d(8‚Üí8, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ
‚îú‚îÄ‚îÄ Conv Block 2 (C2): 32x32 ‚Üí 32x32, RF=9 (Depthwise Separable)
‚îÇ   ‚îú‚îÄ‚îÄ DepthwiseSeparableConv(8‚Üí12, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îî‚îÄ‚îÄ Conv2d(12‚Üí12, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ
‚îú‚îÄ‚îÄ Conv Block 3 (C3): 32x32 ‚Üí 32x32, RF=15 (Dilated Convolution)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(12‚Üí16, 3x3, dilation=2) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îî‚îÄ‚îÄ Conv2d(16‚Üí16, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ
‚îú‚îÄ‚îÄ Conv Block 4 (C40): 32x32 ‚Üí 16x16, RF=21 (Stride=2)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(16‚Üí24, 3x3, stride=2) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îî‚îÄ‚îÄ Conv2d(24‚Üí24, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ
‚îú‚îÄ‚îÄ Conv Block 5: 16x16 ‚Üí 4x4, RF=45 (Optimized: 2 stride=2 operations)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(24‚Üí32, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3, stride=2) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3, stride=2) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îú‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ   ‚îî‚îÄ‚îÄ Conv2d(32‚Üí32, 3x3) + ReLU + BatchNorm + Dropout(0.1)
‚îÇ
‚îú‚îÄ‚îÄ Global Average Pooling: 4x4 ‚Üí 1x1, RF=45
‚îÇ
‚îî‚îÄ‚îÄ Classifier: Linear(32 ‚Üí 10) + LogSoftmax
```

### Detailed Layer Specifications

#### Conv Block 1 (C1) - Standard Convolutions
- **Input**: 3√ó32√ó32
- **Output**: 32√ó32√ó8
- **Receptive Field**: 5
- **Parameters**: ~1K
- **Layers**: 2√ó Conv2d(3x3) + ReLU + BatchNorm + Dropout

#### Conv Block 2 (C2) - Depthwise Separable Convolution
- **Input**: 32√ó32√ó8
- **Output**: 32√ó32√ó12
- **Receptive Field**: 9
- **Parameters**: ~2K
- **Feature**: Depthwise Separable Convolution (reduces parameters)
- **Layers**: DepthwiseSeparableConv + Conv2d + ReLU + BatchNorm + Dropout

#### Conv Block 3 (C3) - Dilated Convolution
- **Input**: 32√ó32√ó12
- **Output**: 32√ó32√ó16
- **Receptive Field**: 15
- **Parameters**: ~8K
- **Feature**: Dilated Convolution (dilation=2)
- **Layers**: Conv2d(dilation=2) + Conv2d + ReLU + BatchNorm + Dropout

#### Conv Block 4 (C40) - Stride=2 Instead of MaxPooling
- **Input**: 32√ó32√ó16
- **Output**: 16√ó16√ó24
- **Receptive Field**: 21
- **Parameters**: ~20K
- **Feature**: Stride=2 convolution (replaces MaxPooling)
- **Layers**: Conv2d(stride=2) + Conv2d + ReLU + BatchNorm + Dropout

#### Conv Block 5 - Optimized: 2 stride=2 operations
- **Input**: 16√ó16√ó24
- **Output**: 4√ó4√ó32
- **Receptive Field**: 45
- **Parameters**: ~66K
- **Feature**: 2 stride=2 operations with minimum 2 convolutions between them
- **Layers**: 10√ó Conv2d(3x3) with 2 stride=2 operations

#### Output Block
- **Global Average Pooling**: 4√ó4√ó32 ‚Üí 1√ó1√ó32
- **FC Layer**: 32 ‚Üí 10
- **Log Softmax**: Final output

### Receptive Field Calculations
The receptive field grows through each layer following the formula: `RF_new = RF_old + (kernel_size - 1) * stride_old`

| Layer | Kernel Size | Stride | Dilation | RF Calculation | RF Value |
|-------|-------------|--------|----------|----------------|----------|
| Input | - | - | - | - | 1 |
| Conv1.1 | 3√ó3 | 1 | 1 | 1 + (3-1)√ó1 | 3 |
| Conv1.2 | 3√ó3 | 1 | 1 | 3 + (3-1)√ó1 | 5 |
| Conv2.1 (Depthwise) | 3√ó3 | 1 | 1 | 5 + (3-1)√ó1 | 7 |
| Conv2.1 (Pointwise) | 1√ó1 | 1 | 1 | 7 + (1-1)√ó1 | 7 |
| Conv2.2 | 3√ó3 | 1 | 1 | 7 + (3-1)√ó1 | 9 |
| Conv3.1 (Dilated) | 3√ó3 | 1 | 2 | 9 + (3-1)√ó2 | 13 |
| Conv3.2 | 3√ó3 | 1 | 1 | 13 + (3-1)√ó1 | 15 |
| Conv4.1 (Stride=2) | 3√ó3 | 2 | 1 | 15 + (3-1)√ó2 | 19 |
| Conv4.2 | 3√ó3 | 1 | 1 | 19 + (3-1)√ó1 | 21 |
| Conv5.1 | 3√ó3 | 1 | 1 | 21 + (3-1)√ó1 | 23 |
| Conv5.2 | 3√ó3 | 1 | 1 | 23 + (3-1)√ó1 | 25 |
| Conv5.3 (Stride=2) | 3√ó3 | 2 | 1 | 25 + (3-1)√ó2 | 29 |
| Conv5.4 | 3√ó3 | 1 | 1 | 29 + (3-1)√ó1 | 31 |
| Conv5.5 | 3√ó3 | 1 | 1 | 31 + (3-1)√ó1 | 33 |
| Conv5.6 | 3√ó3 | 1 | 1 | 33 + (3-1)√ó1 | 35 |
| Conv5.7 (Stride=2) | 3√ó3 | 2 | 1 | 35 + (3-1)√ó2 | 39 |
| Conv5.8 | 3√ó3 | 1 | 1 | 39 + (3-1)√ó1 | 41 |
| Conv5.9 | 3√ó3 | 1 | 1 | 41 + (3-1)√ó1 | 43 |
| Conv5.10 | 3√ó3 | 1 | 1 | 43 + (3-1)√ó1 | 45 |
| GAP | - | - | - | No change | 45 |

### Model Summary
- **Total Parameters**: 105,994 (well under 200k limit)
- **Receptive Field**: 45 (meets >44 requirement)
- **Input Size**: 32x32x3 (CIFAR-10 standard)
- **Output**: 10 classes (CIFAR-10 categories)
- **Architecture Compliance**: ‚úÖ C1C2C3C40 structure
- **Advanced Features**: ‚úÖ Depthwise Separable + Dilated Convolution + Optimized Conv Block 5

## üöÄ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd AU_7_CIFAR

# Install dependencies
pip install -r requirements.txt

# Or use the installation script
chmod +x install.sh
./install.sh
```

### 2. Test the Model Architecture

```bash
# Test model architecture and requirements
python test_model.py

# Test with verbose output
python test_model.py --verbose
```

### 3. Train the Model

```bash
# Train with default settings (up to 100 epochs or test‚â•85% + 3 epochs)
python main.py

# Train with custom parameters
python main.py --epochs 50 --lr 0.1 --batch-size 128

# Test only (no training)
python main.py --test-only
```

### 4. Using Jupyter Notebook

```bash
# Start Jupyter
jupyter notebook

# Open training.ipynb
```

## üìä Usage Examples

### Basic Training

```python
from config import get_config
from src.models import create_model
from src.data import create_data_manager
from src.training import create_trainer
from src.utils import get_device

# Load configuration
config = get_config()

# Setup device
device = get_device()

# Create model
model = create_model(config.model).to(device)

# Setup data
data_manager = create_data_manager(config.data)
train_loader, test_loader = data_manager.setup_data()

# Train model
trainer = create_trainer(model, config.training, device)
metrics = trainer.train(train_loader, test_loader)
```

### Custom Configuration

```python
from config import ModelConfig, DataConfig, TrainingConfig, ProjectConfig

# Create custom configuration
model_config = ModelConfig(
    dropout_rate=0.15,
    c2_out_channels=96,
    c3_dilation=3
)

data_config = DataConfig(
    batch_size=64,
    horizontal_flip_prob=0.3
)

training_config = TrainingConfig(
    epochs=100,
    learning_rate=0.05
)

# Use custom config
config = ProjectConfig(
    model=model_config,
    data=data_config,
    training=training_config
)
```

## üîß Configuration

The project uses a centralized configuration system in `config.py`:

### Model Configuration
```python
@dataclass
class ModelConfig:
    # Model architecture
    input_channels: int = 3
    num_classes: int = 10
    dropout_rate: float = 0.1
    
    # Conv Block channels (optimized for < 200K parameters)
    c1_out_channels: int = 8       # Conv Block 1
    c2_out_channels: int = 12      # Conv Block 2 (Depthwise Separable)
    c3_out_channels: int = 16      # Conv Block 3 (Dilated)
    c4_out_channels: int = 24      # Conv Block 4 (Stride=2)
    c5_out_channels: int = 32      # Conv Block 5 (Optimized with stride=2)
    
    # Special parameters
    c3_dilation: int = 2          # Dilated convolution
    c4_stride: int = 2             # Stride instead of MaxPooling
    fc_hidden_size: int = 32      # FC layer after GAP
    
    # Constraints
    max_parameters: int = 200000   # Parameter limit
    min_receptive_field: int = 44 # RF requirement
```

### Data Configuration
- **Batch Size**: 128
- **Augmentation**: Albumentation with horizontal flip, shift/scale/rotate, coarse dropout
- **Normalization**: CIFAR-10 mean/std values

### Training Configuration
- **Max Epochs**: 100
- **Learning Rate**: 0.1
- **Scheduler**: OneCycleLR
- **Target Test Accuracy**: 85%
- **Post-Target Extra Epochs**: 3

## üèóÔ∏è Architecture Optimization & Training Behavior

The model uses an optimized C1C2C3C40 architecture with the following key features:

### Conv Block 5 Optimization
- **Strategy**: Uses stride=2 after 2 convolutions to reduce spatial dimensions
- **Benefit**: Reduces the number of convolutions needed while maintaining RF > 44
- **Structure**: 16x16 ‚Üí 16x16 ‚Üí 8x8 (after stride=2) ‚Üí 8x8 (additional layers)
- **Receptive Field**: Achieves RF = 45 (> 44 requirement)

### Parameter Efficiency
- **Channel Sizes**: Optimized to keep parameters under 200K
- **Depthwise Separable**: Used in Conv Block 2 for parameter reduction
- **Current Parameters**: ~106K (well under 200K limit)

## üìà Expected Performance
During training, console displays per-epoch accuracies:
- Train Acc
- Test Acc

Smart stopping: runs until 100 epochs or until Test Acc ‚â• 85% and then +3 extra epochs, whichever comes first


- **Parameters**: 105,994 (< 200k requirement ‚úì)
- **Receptive Field**: 45 (> 44 requirement ‚úì)
- **Target Test Accuracy**: 85%+ (with proper training)
- **Training Time**: ~50 epochs with OneCycleLR scheduler

## üß™ Testing

The project includes comprehensive testing:

```bash
# Test model architecture
python test_model.py

# Test with verbose output
python test_model.py --verbose

# Test specific components
python -c "from src.models import create_model; model = create_model(); print('Model test passed!')"
```

## üìä Visualization Features

- **Training Curves**: Loss and accuracy plots
- **Learning Rate Schedule**: LR visualization
- **Sample Images**: Dataset visualization
- **Misclassified Images**: Error analysis
- **Per-Class Accuracy**: Class-wise performance
- **Model Summary**: Architecture overview

## üîç Key Features

### 1. Modular Design
- Separate modules for different functionalities
- Clean interfaces between components
- Easy to extend and modify

### 2. Comprehensive Documentation
- Docstrings for all functions and classes
- Type hints for better code clarity
- Usage examples and explanations

### 3. Configuration Management
- Centralized configuration system
- Easy parameter modification
- Configuration validation

### 4. Advanced Training
- OneCycleLR scheduler for better convergence
- Automatic best model saving
- Comprehensive metrics tracking

### 5. Data Augmentation
- Albumentation library integration
- Configurable augmentation parameters
- Proper normalization

## üõ†Ô∏è Dependencies

- **PyTorch**: Deep learning framework
- **Torchvision**: Computer vision utilities
- **Albumentation**: Advanced data augmentation
- **Matplotlib**: Plotting and visualization
- **Seaborn**: Statistical visualization
- **NumPy**: Numerical computing
- **TQDM**: Progress bars

## üìù License

This project is part of the EVA5 course curriculum and follows the specified requirements for advanced CIFAR-10 classification.

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test the model architecture
5. Submit a pull request

## üìö References

- EVA5 Course for project requirements
- PyTorch documentation
- Albumentation library documentation
- CIFAR-10 dataset paper

## üéâ Acknowledgments

- EVA5 Course for the project requirements
- PyTorch team for the deep learning framework
- Albumentation team for the data augmentation library
- CIFAR-10 dataset creators

---

**Note**: This project successfully meets all specified requirements including the C1C2C3C40 architecture, advanced convolutions, data augmentation, and performance targets while maintaining high code quality and modularity.